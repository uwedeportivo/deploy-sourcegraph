apiVersion: v1
data:
  alert.rules: |
    ALERT PodsMissing
      IF app:up:ratio{app!=""} < 0.9
      FOR 10m
      LABELS { severity="page" }
      ANNOTATIONS {
        summary = "Pods missing from {{`{{`}} $labels.app {{`}}`}}",
        description = "Pods missing from {{`{{`}} $labels.app {{`}}`}}: {{`{{`}} $value {{`}}`}}",
        help = "Alerts when pods are missing.",
      }

    ALERT NoPodsRunning
      IF app:up:ratio{app!=""} < 0.1
      FOR 2m
      LABELS { severity="page" }
      ANNOTATIONS {
        summary = "No pods are running for {{`{{`}} $labels.app {{`}}`}}",
        description = "No pods are running for {{`{{`}} $labels.app {{`}}`}}: {{`{{`}} $value {{`}}`}}",
        help = "Alerts when no pods are running for a service.",
      }

    ALERT ProdPageLoadLatency
      IF histogram_quantile(0.9, sum(rate(src_http_request_duration_seconds_bucket{job="sourcegraph-frontend", route!="raw"}[10m])) by (le)) > 20
      LABELS { severity="page" }
      ANNOTATIONS {
        summary = "High page load latency",
        description = "Page load latency > 20s (90th percentile over all routes; current value: {{`{{`}}$value{{`}}`}}s)",
        help = "Alerts when the page load latency is too high.",
      }

    ALERT GoroutineLeak
      IF go_goroutines >= 10000
      FOR 10m
      ANNOTATIONS {
        summary = "Excessive number of goroutines",
        description = "{{`{{`}} $labels.app {{`}}`}} has more than 10k goroutines. This is probably a regression causing a goroutine leak",
        help = "Alerts when a service has excessive running goroutines.",
      }

    ALERT FSINodesRemainingLow
      IF sum by (instance) (container_fs_inodes_total{pod_name!=""}) > 3000000
      LABELS { severity = "page" }
      ANNOTATIONS {
        summary = "{{`{{`}}$labels.instance{{`}}`}} remaining fs inodes is low",
        description = "{{`{{`}}$labels.instance{{`}}`}} is using {{`{{`}}humanize $value{{`}}`}} inodes",
        help = "Alerts when a node's remaining FS inodes are low.",
      }

    ALERT DiskSpaceLow
      IF node:k8snode_filesystem_avail_bytes:ratio < 0.10
      ANNOTATIONS {
        summary = "{{`{{`}}$labels.exported_name{{`}}`}} has less than 10% available disk space",
        help = "Alerts when a node has less than 10% available disk space.",
      }

    ALERT DiskSpaceLowCritical
      IF node:k8snode_filesystem_avail_bytes:ratio{exported_name=~".*prod.*"} < 0.05
      LABELS { severity="page" }
      ANNOTATIONS {
        summary = "Critical! {{`{{`}}$labels.exported_name{{`}}`}} has less than 5% available disk space",
        help = "Alerts when a node has less than 5% available disk space.",
      }

    ALERT SearcherErrorRatioTooHigh
      IF searcher_errors:ratio10m > 0.1
      FOR 20m
      ANNOTATIONS {
        summary = "Error ratio exceeds 10%",
        help = "Alerts when the search service has more than 10% of requests failing.",
      }

    # TODO(sqs): enable this after we have tested it in prod, to avoid needless alerts
    #
    # ALERT SymbolsErrorRatioTooHigh
    #   IF symbols_errors:ratio10m > 0.1
    #   FOR 20m
    #   ANNOTATIONS {
    #     summary = "Error ratio exceeds 10%",
    #     help = "Alerts when the symbols service has more than 10% of requests failing.",
    #   }

    # http_response_size_bytes is measured at the Prometheus clients, not at the server.
    ALERT PrometheusMetricsBloat
      IF http_response_size_bytes{handler="prometheus", quantile="0.5", job!="kubernetes-nodes", job!="kubernetes-apiservers"} > 20000
      ANNOTATIONS {
        summary = "{{`{{`}}$labels.job{{`}}`}} in {{`{{`}}$labels.ns{{`}}`}} is probably leaking metrics (unbounded attribute)",
        help = "Alerts when a service is probably leaking metrics (unbounded attribute).",
      }

  extra.rules: ""
  node.rules: |
    node:container_cpu_usage_seconds_total:ratio_rate5m =
      sum by (instance) (rate(container_cpu_usage_seconds_total{kubernetes_pod_name=""}[5m]))
      /
      max by (instance) (machine_cpu_cores)

    task:container_memory_usage_bytes:max = max by (namespace, container_name)(container_memory_usage_bytes{container_name!=""})
    task:container_cpu_usage_seconds_total:sum = sum by (id, namespace, container_name) (irate(container_cpu_usage_seconds_total{container_name!=""}[1m]))

    node:k8snode_filesystem_avail_bytes:ratio =
      min by (exported_name) (k8snode_filesystem_avail_bytes / k8snode_filesystem_size_bytes)

  searcher.rules: |
    ##########
    # Errors #
    ##########

    searcher_requests:rate10m = sum by (instance)(rate(searcher_service_request_total[10m]))
    searcher_errors:rate10m   = sum by (instance)(rate(searcher_service_request_total{code!="200",code!="canceled"}[10m]))
    searcher_errors:ratio10m = sum(searcher_errors:rate10m) / sum(searcher_requests:rate10m)
  sourcegraph.rules: |
    # This files contains recording rules targeted at the sourcegraph-frontend app

    # Duration (5m)
    task:src_http_request_duration_seconds_bucket:rate5m = rate(src_http_request_duration_seconds_bucket{job=~"sourcegraph-.*"}[5m])
    route:src_http_request_duration_seconds_bucket:rate5m = sum by (route, ns, le)(task:src_http_request_duration_seconds_bucket:rate5m)
    job:src_http_request_duration_seconds_bucket:rate5m = sum by (ns, le)(task:src_http_request_duration_seconds_bucket:rate5m)
    prod:src_http_request_duration_seconds_bucket:rate5m = sum by (le)(task:src_http_request_duration_seconds_bucket:rate5m)

    # Counts (5m)
    task:src_http_request_count:rate5m = rate(src_http_request_duration_seconds_count{job=~"sourcegraph-.*"}[5m])
    route:src_http_request_count:rate5m = sum by (route, code, ns)(task:src_http_request_count:rate5m)
    job:src_http_request_count:rate5m = sum by (code, ns)(task:src_http_request_count:rate5m)
    prod:src_http_request_count:rate5m = sum by (code)(task:src_http_request_count:rate5m)

    # Duration (30m)
    task:src_http_request_duration_seconds_bucket:rate30m = rate(src_http_request_duration_seconds_bucket{job=~"sourcegraph-.*"}[30m])
    route:src_http_request_duration_seconds_bucket:rate30m = sum by (route, ns, le)(task:src_http_request_duration_seconds_bucket:rate30m)
    job:src_http_request_duration_seconds_bucket:rate30m = sum by (ns, le)(task:src_http_request_duration_seconds_bucket:rate30m)
    prod:src_http_request_duration_seconds_bucket:rate30m = sum by (le)(task:src_http_request_duration_seconds_bucket:rate30m)

    # Counts (30m)
    task:src_http_request_count:rate30m = rate(src_http_request_duration_seconds_count{job=~"sourcegraph-.*"}[30m])
    route:src_http_request_count:rate30m = sum by (route, code, ns)(task:src_http_request_count:rate30m)
    job:src_http_request_count:rate30m = sum by (code, ns)(task:src_http_request_count:rate30m)
    prod:src_http_request_count:rate30m = sum by (code)(task:src_http_request_count:rate30m)


    # Perf targets are over a day
    prod:src_http_request_duration_seconds_bucket:rate1d = sum by (route, le)(rate(src_http_request_duration_seconds_bucket{job=~"sourcegraph-.*"}[1d]))


    # Measure uptime of services
    app:up:sum = sum by (app)(up)
    app:up:count = count by (app)(up)
    app:up:ratio =
      app:up:sum
        / on (app)
      app:up:count
  symbols.rules: |
    ##########
    # Errors #
    ##########

    symbols_requests:rate10m = sum by (instance)(rate(symbols_service_request_total[10m]))
    symbols_errors:rate10m   = sum by (instance)(rate(symbols_service_request_total{code!="200",code!="canceled"}[10m]))
    symbols_errors:ratio10m = sum(symbols_errors:rate10m) / sum(symbols_requests:rate10m)
kind: ConfigMap
metadata:
  labels:
    deploy: prometheus
  name: prometheus
